# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ohxWE-rDLL2KbMoVkCw_eFQVc5rOLcI2
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import mpl_toolkits
# %matplotlib inline

#from google.colab import drive

#drive.mount('/content/drive' , force_remount=True)

#data =  pd.read_csv('/content/drive/MyDrive/DataSets/Egypt_villas_Price.csv', encoding='latin-1')
data = pd.read_csv('Egypt_villas_Price.csv')
data.head()

#encoding
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
#data.City = le.fit_transform(data.City)
data.Security = le.fit_transform(data.Security)
data.Balcony = le.fit_transform(data.Balcony)
data.Private_Garden = le.fit_transform(data.Private_Garden)
data.Pets_Allowed = le.fit_transform(data.Pets_Allowed)
data.Covered_Parking = le.fit_transform(data.Covered_Parking)
data.Maids_Room = le.fit_transform(data.Maids_Room)
data.Electricity_Meter = le.fit_transform(data.Electricity_Meter)
data.Natural_Gas = le.fit_transform(data.Natural_Gas)
data.Landline = le.fit_transform(data.Landline)
data.Pool = le.fit_transform(data.Pool)
data.Central_heating = le.fit_transform(data.Central_heating)
data.Built_in_Kitchen_Appliances = le.fit_transform(data.Built_in_Kitchen_Appliances)
data.Elevator = le.fit_transform(data.Elevator)
print(data.head())



# Calculate the IQR of the price column
Q1 = data['Price'].quantile(0.25)
Q3 = data['Price'].quantile(0.75)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Find the outliers in the price column
outliers = data[(data['Price'] < lower_bound) | (data['Price'] > upper_bound)]

# Print the outliers
print(len(outliers))
data = data[(data['Price'] >= lower_bound) & (data['Price'] <= upper_bound)]

data.Bathrooms = data.Bathrooms.str.replace('+','1')
data.Bedrooms = data.Bedrooms.str.replace('+','1')

data['Bathrooms'] = data['Bathrooms'].astype(float) 
data['Bedrooms'] = data['Bedrooms'].astype(float) 
data['Area'] = data['Area'].str.replace(',', '')
data['Area'] = data['Area'].astype(float) 
data = data.rename(columns={'ï»¿Type': 'Type'})
data.dtypes

#scaling the area column
from sklearn.preprocessing import StandardScaler

# Create a scaler object
scaler = StandardScaler()

# Select the column to scale
area = data[['Area']]

# Fit the scaler to the data
scaler.fit(area)

# Scale the data
area_scaled = scaler.transform(area)

# Replace the original 'area' column with the scaled values
data['Area'] = area_scaled

# Print the first 5 rows of the data
print(data.head())

#mapping values in the payment method column
data['Payment_Option'].unique()

# Define a dictionary to map the old values to the new values
value_map = {'Cash or Installment': 2, 'Cash': 1,'Installment':0,'Unknown':-1}

# Replace the values in the 'availability' column with 0 and 1
data['Payment_Option'] = data['Payment_Option'].replace(value_map)
data['Payment_Option'].unique()

#mapping values in the furnished column
data['Furnished'].unique()

# Define a dictionary to map the old values to the new values
value_map = {'Yes': 1,'Unknown':0,'No':0}

# Replace the values in the 'availability' column with 0 and 1
data['Furnished'] = data['Furnished'].replace(value_map)

#mapping values in the Type column
data['Type'].unique()

# Define a dictionary to map the old values to the new values
value_map = {'Stand Alone Villa': 1,'Town House':0,'Twin House':2}

# Replace the values in the 'availability' column with 0 and 1
data['Type'] = data['Type'].replace(value_map)
data['Type'].unique()

"""
#mapping values in the delivery term column
data['Delivery_Term'].unique()

# Define a dictionary to map the old values to the new values
value_map = {'Finished': 1*6,'Unknown':0,'Not Finished':0,'Semi Finished':3,'Core & Shell':1}

# Replace the values in the 'availability' column with 0 and 1
data['Delivery_Term'] = data['Delivery_Term'].replace(value_map)
data['Delivery_Term'].unique()
"""

#using alternative method for mapping the delivery term column

# perform one-hot encoding on the delivery date column
delv_term_dummies = pd.get_dummies(data['Delivery_Term'], prefix='Delivery_Term')

# concatenate the one-hot encoded Compound columns with the original dataframe
data_encoded = pd.concat([data, delv_term_dummies], axis=1)

# drop the original Compound column
data_encoded = data_encoded.drop('Delivery_Term', axis=1)
print(data_encoded.shape[1])

"""
#mapping values in the delivery date column
data['Delivery_Date'].unique()

# Define a dictionary to map the old values to the new values
value_map = {'2027': 1*6,'2026':2*6,'2025':3*6,'2024':4*6,'2023':5*6,'2022':6*6,'Unknown':0,'within 6 months':7*6,'Ready to move':100,'soon':0}

# Replace the values in the 'availability' column with 0 and 1
data['Delivery_Date'] = data['Delivery_Date'].replace(value_map)
data['Delivery_Date'].unique()
"""

#using alternative method for mapping the delivery date column

# perform one-hot encoding on the delivery date column
delv_date_dummies = pd.get_dummies(data['Delivery_Date'], prefix='Delivery_Date')

# concatenate the one-hot encoded Compound columns with the original dataframe
data_encoded = pd.concat([data_encoded, delv_date_dummies], axis=1)

# drop the original Compound column
data_encoded = data_encoded.drop('Delivery_Date', axis=1)
print(data_encoded.shape[1])

# perform one-hot encoding on the Compound column
compound_dummies = pd.get_dummies(data['Compound'], prefix='compound')

# concatenate the one-hot encoded Compound columns with the original dataframe
data_encoded = pd.concat([data_encoded, compound_dummies], axis=1)

# drop the original Compound column
data_encoded = data_encoded.drop('Compound', axis=1)
print(data_encoded.shape[1])

# perform one-hot encoding on the city column
city_dummies = pd.get_dummies(data['City'], prefix='City')

# concatenate the one-hot encoded city columns with the original dataframe
data_encoded2 = pd.concat([data_encoded, city_dummies], axis=1)

# drop the original Compound column
data_encoded2 = data_encoded2.drop('City', axis=1)
print(data_encoded2.shape[1])

# data=data.loc[data["Furnished"]!="Unknown"]
# data["Furnished"]

data.head()

data.isnull().sum().sum()

Y = data_encoded2.Price
# includes the fields other than prices
#X = data.iloc[:,1:]
#X = data_encoded.loc[:,[2,3,4,6,7,12,13,14,15,16,17,18,19,20,21]]
#X = data[['Bedrooms', 'Bathrooms', 'Area', 'Compound', 'Payment_Option', 'Security', 'Balcony', 'Private_Garden', 'Pets_Allowed', 'Covered_Parking', 'Maids_Room', 'Electricity_Meter', 'Natural_Gas', 'Landline', 'Pool','Central_heating','Built_in_Kitchen_Appliances','Elevator']]
X = data_encoded2.iloc[:, [0,2, 3,4,5,6] + list(range(8, len(data_encoded2.columns)))]
#X=data.loc[data["Area"]!="111"]
print(data_encoded2.columns[7])

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.20 ,random_state=42)

#from sklearn.linear_model import LinearRegression
#regression = LinearRegression()
#regression.fit(x_train,y_train)

import xgboost as xgb
import matplotlib.pyplot as plt

# Define the XGBoost model with hyperparameters
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=5000, max_depth=12, learning_rate=0.01, subsample=0.5, colsample_bytree=0.8, seed=42)

# Fit the model on the training set
xgb_model.fit(x_train, y_train)

# Make predictions on the testing set
#y_prediction = xgb_model.predict(x_test)

# Visualize the first tree in the model
# Plot the first tree
#fig, ax = plt.subplots(figsize=(30, 30))
#xgb.plot_tree(xgb_model, num_trees=0, ax=ax)
#plt.show()
# Save the plot as a PDF file
#plt.savefig('/content/drive/MyDrive/DataSets/xgboost_tree.pdf')

"""
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# Define the hyperparameter space to search over
param_distributions = {
    'n_estimators': randint(1000, 5000),
    'max_depth': randint(6, 20),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.5, 0.5),
    'colsample_bytree': uniform(0.5, 0.5),
}

# Create an instance of the XGBoost regressor
model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)

# Perform randomized search with cross-validation
random_search = RandomizedSearchCV(
    model,
    param_distributions,
    n_iter=50,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1,
    random_state=42,
)

# Fit the randomized search to the training data
random_search.fit(x_train, y_train)

# Print the best hyperparameters and corresponding score
print("Best hyperparameters:", random_search.best_params_)
print("Best R2 score:", random_search.best_score_)
"""

#Linear Regression
#from sklearn.linear_model import LinearRegression
#regression = LinearRegression()
#regression.fit(x_train,y_train)
#y_prediction =  regression.predict(x_test)

#polynomial model
#from sklearn.preprocessing import PolynomialFeatures
#from sklearn.linear_model import LinearRegression

#Create polynomial features object
#poly = PolynomialFeatures(2)

#Fit polynomial features object to training set
#X_train_poly = poly.fit_transform(x_train)

#Create linear regression object
#model = LinearRegression()

#Fit linear regression object to training set
#model.fit(X_train_poly, y_train)

# Transform testing set using polynomial features object
#X_test_poly = poly.transform(x_test)

#Use linear regression object to predict target variable for testing set
#y_prediction = model.predict(X_test_poly)

#randome forest model
#from sklearn.ensemble import RandomForestRegressor

# Define the Random Forest model with hyperparameters
#rf_model = RandomForestRegressor(n_estimators=2000, max_depth=7, random_state=42)

# Fit the model on the training set
#rf_model.fit(x_train, y_train)

# Make predictions on the testing set
#y_prediction = rf_model.predict(x_test)

#y_prediction =  regression.predict(x_test)
#print(x_test)
#print(y_test)
#print(y_prediction[1000])



#regression.predict([[4,3,224,100]])

#regression.score(X,Y)

#import sklearn.metrics as sm
#print("Mean absolute error =", round(sm.mean_absolute_error(y_test, y_prediction), 2)) 
#print("Mean squared error =", round(sm.mean_squared_error(y_test, y_prediction), 2)) 
#print("Median absolute error =", round(sm.median_absolute_error(y_test, y_prediction), 2)) 
#print("Explain variance score =", round(sm.explained_variance_score(y_test, y_prediction), 2)) 
#print("R2 score =", round(sm.r2_score(y_test, y_prediction), 2))

import joblib
joblib.dump(xgb_model, 'model.pkl')
print("Model dumped!")

# Load the model that you just saved
lr = joblib.load('model.pkl')

# Saving the data columns from training
model_columns = list(X.columns)
joblib.dump(model_columns, 'model_columns.pkl')
print("Models columns dumped!")